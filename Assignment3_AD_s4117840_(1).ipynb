{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z81sDP-WhbM4"
   },
   "source": [
    "# Assignment 3: LDA Topic Modeling\n",
    "\n",
    "## Note\n",
    "Installing Tomotopy locally can return an error, if that's the case run this notebook on Google Colab\n",
    "\n",
    "## Research Background\n",
    "\n",
    "LDA is a popular topic modeling algorithm widely used in the fields of Digital Humanities and Social Sciences. In the field of political communication, topic modeling is often applied for analyzing politicians Twitter/X posts, identitying thematic patterns or topics revolving around their posts.\n",
    "\n",
    "For this assignment, students will work with tweets from two USA politicians, Donald Trump and Bernie Sanders, who are often regarded as right-wing populist and left-wing populist respectively. Right-wing populism often emphasizes nationalism, anti-immigration policies, and a critique of global elites from a culturally conservative perspective, focusing on preserving traditional values and social hierarchies. Left-wing populism prioritizes economic inequality, advocating for the redistribution of wealth, expansion of social services, and empowerment of the working class against the capitalist elite. While both forms of populism appeal to the \"common people\" against perceived elites and established structures, they diverge significantly in their identification of the elites, proposed solutions, and core ideologies. For a more detailed explanation, you can read the chapter by Macaulay (2019) \"Bernie and The Donald: A comparison of left-and right-wing populist discourse\" (full reference below).\n",
    "\n",
    "**Research Questions**\n",
    "1. What topics are revolving around Donald Trump and Bernie Sanders' posts separately?\n",
    "2. What are the topic differences between Trump (right-wing popoulist) and Sanders (left-wing populist)?\n",
    "\n",
    "**Aim:**\n",
    "1. The first aim of the assignment is to conduct LDA topic modeling. Identify thematic patterns or politics revolving around Trump or Sanders's posts.\n",
    "2. The second aim is to critically evaluate the results of topic modeling. Try different numbers of topics to see with which settings the topics are more coherent. Critically reflect on the results of LDA topic modeling, discussing them in relation to existing theories about populism.\n",
    "\n",
    "**Data**\n",
    "Two datasets are prepared for this assginment. Tweets from Trump and tweets from Sanders. Students are asked to work on these two datasets.\n",
    "\n",
    "**Methods**\n",
    "1. Word segamentation\n",
    "2. Removing stopwords\n",
    "3. LDA topic modeling\n",
    "4. Topic evaulation (coherence and human evaluation)\n",
    "5. Visualization of results.\n",
    "\n",
    "**References**\n",
    "1. Macaulay, M. (2019). Bernie and the Donald: A comparison of Left-and Right-wing populist discourse. *Populist discourse: International perspectives*, 165-195.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOAzU-HhhbM9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4wYU9mJM99W"
   },
   "source": [
    "### Q1. Install necessary libraries, including `tomotopy` and `little_mallet_wrapper`, and import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFH5FrRzhbM-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Q1 (code)\n",
    "!pip install tomotopy\n",
    "!pip install little_mallet_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import little_mallet_wrapper\n",
    "import seaborn\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4LzJUC3hbNC"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1qdFBgYM3_Z"
   },
   "source": [
    "### Q2. Load the two datasets and concatenate them\n",
    "\n",
    "The goal is to run topic modelling on the combined dataset of Sanders and Trump's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gc1Wj2K3hbNE"
   },
   "outputs": [],
   "source": [
    "# Q2 (code)\n",
    "directory = \"/\"\n",
    "files = glob.glob(f\"{directory}/*.csv\")\n",
    "\n",
    "sanders = pd.read_csv('sanders_tweets.csv')\n",
    "trump = pd.read_csv('trump_tweets.csv')\n",
    "\n",
    "df = pd.concat([sanders, trump], ignore_index=True) # Concatenate the datasets\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr84dAOvhbNE"
   },
   "source": [
    "### Q3. Clean the data\n",
    "\n",
    "Transform all tweets to lowercase, remove stopwords, punctuation, and numbers. Add the processed text to a list called `training_data`.\n",
    "Create a list with the content of the tweets (`original_texts`) and a list that allows you to identify both the author of the tweet and its ID (`titles`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 (code)\n",
    "\n",
    "import re\n",
    "\n",
    "training_data = [\n",
    "    re.sub(r\"http\\S+|www\\S+|https\\S+|\\/\\/t|co\\/|\\@\\w+|realdonaldtrump\", '',\n",
    "           little_mallet_wrapper.process_string(text, numbers='remove'), flags=re.MULTILINE)\n",
    "    for text in df['Content']\n",
    "] # Lowers the text and removes stopwords, punctuation, and numbers via little_mallet_wrapper.process_string()\n",
    "# added \"processed_text = re.sub(r\"http\\S+|www\\S+|https\\S+|\\/\\/t|co\\/|\\@\\w+|realdonaldtrump\", '', processed_text, flags=re.MULTILINE)\"\n",
    "# to remove URLS and mentions\n",
    "\n",
    "original_texts = [text for text in df['Content']] # Add 'Content' to original_texts\n",
    "titles = [title for title in df['Title']] # Add 'Title' to titles\n",
    "\n",
    "little_mallet_wrapper.print_dataset_stats(training_data) # Get training data summary statistics\n",
    "\n",
    "## Code adapted from [https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/10-Topic-Modeling-CSV.html#process-reddit-posts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeKlV0IehbNH"
   },
   "source": [
    "## LDA topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhTQrfCkHbXS"
   },
   "source": [
    "### Q4. Train a an LDA topic model with `tomotopy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKrCy-P6hbNI"
   },
   "outputs": [],
   "source": [
    "# Q4 (code)\n",
    "\n",
    "# Number of topics to return\n",
    "num_topics = 15\n",
    "# Numer of topic words to print out\n",
    "num_topic_words = 10\n",
    "\n",
    "# Intialize the model\n",
    "model = tp.LDAModel(k=num_topics)\n",
    "\n",
    "# Add each document to the model, after splitting it up into words\n",
    "for text in training_data:\n",
    "    model.add_doc(text.strip().split())\n",
    "\n",
    "print(\"Topic Model Training...\\n\\n\")\n",
    "# Iterate over the data 10 times\n",
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    model.train(iterations)\n",
    "    print(f'Iteration: {i}\\tLog-likelihood: {model.ll_per_word}')\n",
    "\n",
    "## Code taken from \"Topic_Modeling_tomotopy.ipynb\" from Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gucG5gYghbNI"
   },
   "source": [
    "### Q5. Print out the top words for each topic and manually evaluate their coherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fu_F1j17hbNJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Q5a (code)\n",
    "\n",
    "# Print out top 10 words for each topic\n",
    "print(\"\\nTopic Model Results:\\n\\n\")\n",
    "\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())\n",
    "    print(f\"✨Topic {topic_number}✨\\n\\n{topic_words}\\n\")\n",
    "\n",
    "## Code taken from \"Topic_Modeling_tomotopy.ipynb\" from Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSeonT734KsL"
   },
   "source": [
    "_# Q5b (words)_\n",
    "\n",
    "_# Describe what each topic is about. What ideas, values, or situations do these keywords refer to?_\n",
    "\n",
    "Because the topics can get shifted around a little when re-running the code, I have added what a keywords show up per topic as I was answering this question.\n",
    "\n",
    "* **Topic 0** (_trump court supreme president foxnews watch via judge tonight kavanaugh_): This likely refers to \"trump\"'s nomination of nominated Brett \"kavanaugh\" to the \"supreme\" \"court\". Kavanaugh was previously a U.S. circuit \"judge\".\n",
    "* **Topic 1** (_great vote total endorsement strong complete military amendment state job_): These words seem mostly positive, based on \"great\", \"endorsement\", \"strong\", and \"complete\". This topic seems to be praising the US/state military.\n",
    "* **Topic 2** (_president thank american trump today whitehouse coronavirus america covid new_): This refers to the \"covid\" / \"coronavirus\" outbreak which happened in \"america\" during \"president\" \"trump\"'s term. \n",
    "* **Topic 3** (_tax workers people must working billion pay change americans million_): Sanders is infamous for wanting to \"tax\" \"million\"aires/\"billion\"aires more, so blue collar \"workers\" / the average \"working\" \"american\" would benefit.\n",
    "* **Topic 4** (_great thank today white house president honor whitehouse day united_): This seems to be a rather broad topic referring to praising either the \"president\" and/or the \"white\" \"house\" / \"whitehouse\" of the \"united\" States of America.\n",
    "* **Topic 5** (_trump fbi president schiff mueller campaign russia call report comey_): This refers to the _Robert \"mueller\" special counsel investigation_, which was an \"fbi\" investigatio into the idea that \"president\" \"trump\"'s presidential \"campaign\" had either been sponsored by or supported by \"russia\". Adam \"schiff\" was one of the first persons to accuse Trump of this.\n",
    "* **Topic 6** (_biden joe president trump democrats would never nothing left years_): This refers to \"joe\" \"biden\" taking over as \"president\" after \"trump\"'s term was up. Biden is a \"democrat\". \n",
    "* **Topic 7** (_jobs china years trade economy great ever year big country_): Both Trump and Sanders wanted more \"jobs\" in the \"country\" to boost the American \"economy\". Trump also wanted a \"trade\" deal with China.\n",
    "* **Topic 8** (_thank trump great big night maga last win republican see_): This likely has to do with the 2016 election results, as the words seem to indicate a \"great\" \"republican\" \"win\", and it being a \"big\" \"night\" for the \"trump\" / \"maga\" ('Make America Great Again') campaign.\n",
    "* **Topic 9** (_health care people must right americans country medicare security social_): Sanders especially advocated that \"people\" / \"americans\" should have the right to affordable \"health\" \"care\", wuch as \"medicare\". \n",
    "* **Topic 10** (_border wall great security north country korea immigration many good_): This topic seems to relate to foreign policy, specifically \"immigration\". Notably, Trump very much wanted to close the \"border\"s, while Sanders did not. Trump is also infamous for wanting to build a \"wall\" on the border between the USA and Mexico. Moreover, it has been alleged that Trump had ties to \"north\" \"korea\". \n",
    "* **Topic 11** (_news fake media people great new cnn never even many_): This topic is likely about the concept of \"fake\" \"media\" / \"fake\" \"news\" during the election, in which Trump called for \"people\" to stop believing said media. Trump specifically disliked \"cnn\", and promised to make America \"great\" again.\n",
    "* **Topic 12** (_democrats senate vote house republicans impeachment people republican pelosi get_): This refers to Trump's \"impeachment\" by the United States congress. The \"house\" \"democrats\" \"vote\"d to impeach Trump, under Nancy \"pelosi\" -- twice. However, the \"senate\" acquitted Trump both times.\n",
    "* **Topic 13** (_people must law country police nation american stand justice americans_): A current trend in America is the call for equal \"justice\", as the \"country\"'s \"law\"s are considered to be unjust and biassed towards a large group of \"americans\". Most notably, the \"police\" force is considered to be racist, incompetent, and power hungry -- unrepresentative of the \"nation\".\n",
    "* **Topic 14** (_war president trump united states iran live world whitehouse must_): This most likely refers to the \"war\" in \"iran\" which the \"united\" \"states\" involved themselves in under \"president\" \"trump\" in the \"whitehouse\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_Cv23XVAj8E"
   },
   "source": [
    "## Topic coherence\n",
    "\n",
    "Use `tomotopy`'s [`.coherence()`](https://bab2min.github.io/tomotopy/v0.10.0/en/coherence.html) function to automatically calculate the topic coherence.\n",
    "\n",
    "The coherence value can vary from `0` (no coherence) to `1` (maximum coherence). Interpret the results and, if needed, retrain the model using a different number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzDijy831LHb"
   },
   "outputs": [],
   "source": [
    "# There are different metrics for coherence, we choose `c_v`\n",
    "\n",
    "coh = tp.coherence.Coherence(model, coherence='c_v')\n",
    "average_coherence = coh.get_score()\n",
    "coherence_per_topic = [coh.get_score(topic_id=k) for k in range(model.k)]\n",
    "\n",
    "print('==== Coherence : {} ===='.format('c_v'))\n",
    "print('Average:', average_coherence, '\\nPer Topic:', coherence_per_topic)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrEHnkB-Cs4_"
   },
   "source": [
    "### Q6. Interpret topic coherence\n",
    "\n",
    "Report the following:\n",
    "- number of topics you initially used to train the model and the coherence score you got\n",
    "- changes made to the number of topics and new coherence scores obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_# Q6 (words)_\n",
    "\n",
    "Topic coherence is \"a measure that quantifies the semantic similarity between high scoring words in each topic\" (Week 6 Slides). In this instance, `0` means there is no coherence, while `1` means there is maximum coherence.\n",
    "\n",
    "At first, I used **15** topics to train the model on (see: _Q5b_), which led to the following coherence scores:\n",
    "\n",
    "* Average: `0.5632082928717137`\n",
    "* Per Topic: `[0.5458732128143311, 0.7524268865585327, 0.4849416330456734, 0.6362292140722274, 0.5286169499158859, 0.6557234644889831, 0.5389063686132431, 0.5345719367265701, 0.4716217666864395, 0.6242271333932876, 0.5291290327906608, 0.5329573504626751, 0.5582489460706711, 0.5039236009120941, 0.5507268965244293]`\n",
    "\n",
    "These coherence scores are average across the board, with a `0.56` average.\n",
    "\n",
    "With **7** topics, the coherence scores are as such:\n",
    "\n",
    "* Average: `0.5050233439675399` \n",
    "* Per Topic: `[0.6074946105480195, 0.41739652007818223, 0.5534183442592621, 0.5261138953268528, 0.4303628757596016, 0.46238944977521895, 0.5379877120256424]`\n",
    "\n",
    "Similar to the coherence scores of the 15 topics, these seem to be rather average as well, with a `0.51` average.\n",
    "\n",
    "Seeing as less topics also led to a lower overall score, I re-ran the code again with **20** topics as well. These coherence scores were as follows:\n",
    "\n",
    "* Average: `0.6019040995649994`\n",
    "* Per Topic: `[0.5379222705960274, 0.6452826857566833, 0.7054191589355469, 0.5911167889833451, 0.5444848254323006, 0.6111246004700661, 0.5678991347551345, 0.5200339704751968, 0.5672390341758728, 0.5575328394770622, 0.49784696102142334, 0.7895608007907867, 0.5890060167759656, 0.574989914894104, 0.5100615963339805, 0.5470394521951676, 0.5870368123054505, 0.7751894354820251, 0.655393123626709, 0.6639025688171387]`\n",
    "\n",
    "As suspected, a higher number of topics also increases the coherence scores, this time with a `0.6` average. To prove this in a rather aggregious way, I also ran the code with **50** topics, leading to these coherence scores:\n",
    "\n",
    "* Average: `0.6311494893133641`\n",
    "* Per Topic: `[0.6836634904146195, 0.8209899723529815, 0.6923168256878853, 0.7311984956264496, 0.5584407389163971, 0.6177050292491912, 0.5930688142776489, 0.6683756291866303, 0.7092124283313751, 0.5776619702577591, 0.6318268001079559, 0.5643571749329567, 0.5979069232940674, 0.5825867474079132, 0.5703236669301986, 0.6193717330694198, 0.7092808006331325, 0.7299154698848724, 0.7189078152179718, 0.5304928898811341, 0.5567850947380066, 0.6814566493034363, 0.594406220316887, 0.6663949221372605, 0.5368732929229736, 0.6486244201660156, 0.5669801160693169, 0.624530678987503, 0.6239861393347382, 0.6417555302381516, 0.7629347383975983, 0.6618366226553917, 0.6887161530554294, 0.7351990342140198, 0.5312954694032669, 0.5753773808479309, 0.6305660545825958, 0.648004800081253, 0.5762645572423934, 0.6035854101181031, 0.6007377147674561, 0.6145432263612747, 0.5176980793476105, 0.7588082164525985, 0.568218144774437, 0.5892994463443756, 0.6328431844711304, 0.5624637380242348, 0.7445100963115692, 0.505175918340683]`\n",
    "\n",
    "This once again shows an increase, with a `0.63` average. However, the increase is much smaller than the jump from 7 to 20 topics, or even 15 to 20 topics. This means that there is a curve in relation to the coherence scores. I also assume that the inclusion of more topics is related to the medium of the corpus, as the corpus consists of over 3000 messages short messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpZbWHxfBr1E"
   },
   "source": [
    "### X1. Optional question 1\n",
    "(This question is not compulsory, it only allows you to get an extra point.)\n",
    "\n",
    "Create a function to plot the average coherence for models with different number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx124d3M5P4R"
   },
   "outputs": [],
   "source": [
    "# X1 (code)\n",
    "# Tip: y = average topic coherence; x = number of topics in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYY_TIOyDLnN"
   },
   "source": [
    "### Q7. Topic distributions\n",
    "Calculate the topic distributions for all tweets and get the top documents for some topics (between 2 and 5) that you think could be more representative of Sanders or Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4CQGxl6UrfL"
   },
   "outputs": [],
   "source": [
    "# Q7a (code)\n",
    "\n",
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "# This list corresponds to the likelihood that each of the 15 topics exists in a tweet\n",
    "\n",
    "topic_distributions[0]\n",
    "# '0' taken from [https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/10-Topic-Modeling-CSV.html#load-topic-distributions]\n",
    "\n",
    "## Code taken from \"Topic_Modeling_tomotopy.ipynb\" from Week 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def make_md(string):\n",
    "    display(Markdown(str(string)))\n",
    "\n",
    "def get_top_docs(docs, topic_distributions, topic_index, n=5):\n",
    "\n",
    "    sorted_data = sorted([(_distribution[topic_index], _document)\n",
    "                          for _distribution, _document\n",
    "                          in zip(topic_distributions, docs)], reverse=True)\n",
    "\n",
    "    topic_words = topics[topic_index]\n",
    "\n",
    "    make_md(f\"### ✨Topic {topic_index}✨\\n\\n{topic_words}\\n\\n\")\n",
    "    print(\"---\")\n",
    "\n",
    "    for probability, doc in sorted_data[:n]:\n",
    "        # Make topic words bolded\n",
    "        for word in topic_words.split():\n",
    "            if word in doc.lower():\n",
    "                doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc, re.IGNORECASE)\n",
    "\n",
    "        make_md(f'✨  \\n**Topic Probability**: {probability}  \\n**Document**: {doc}\\n\\n')\n",
    "\n",
    "    return\n",
    "\n",
    "## Code taken from \"Topic_Modeling_tomotopy.ipynb\" from Week 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_top_docs(titles, topic_distributions, topic_index=2, n=7) # Display 7 documents for topic 2\n",
    "get_top_docs(titles, topic_distributions, topic_index=4, n=7) # Display 7 documents for topic 4\n",
    "get_top_docs(titles, topic_distributions, topic_index=7, n=7) # Display 7 documents for topic 7\n",
    "get_top_docs(titles, topic_distributions, topic_index=10, n=7) # Display 7 documents for topic 10\n",
    "get_top_docs(titles, topic_distributions, topic_index=14, n=7) # Display 7 documents for topic 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nyQ7GnIDrWr"
   },
   "source": [
    "Interpret the results above. Are there topics that have top tweets only by one politician? Why do you think these topics are more representative of one of the two politicians' views?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNXy1koX6sFN"
   },
   "source": [
    "_# Q7b (words)_\n",
    "\n",
    "Because the topics can get shifted around a little when re-running the code, I have added what a keywords show up per topic as I was answering this question. (The topics are the same as _Q5b_.) It should be noted that because I wanted to keep the topics the same as Q5b, and had messed around with the coherence scores prior to this, I was unable to re-run this code without getting different topic orders and/or slightly different topic words. As such, Trump is overrepresented in these results.\n",
    "\n",
    "* **Topic 2** (_president thank american trump today whitehouse coronavirus america covid new_): The covid-related topic is primarily related to Trump (none of Sanders' tweets have made it into the top 5). This likely stems from Trump having been president during the covid period and thus needing to speak on it. In contrast, Sanders was not the leader of the United States of America and likely had much less to say about it -- this is not saying he did not speak about it at all. \n",
    "* **Topic 4** (_great thank today white house president honor whitehouse day united_): Topic 4 is also populated by Trump's tweets, which enforced the idea that the topic relates to his precidency. \n",
    "* **Topic 7** (_jobs china years trade economy great ever year big country_): I must admit I'm a little surprised that Sanders did not show up in these tweets at all, as I figured he would show up in the top results based on his commentary on the current US job market and the economy. Instead, it seems the top posts are related to Trump's policy of wanting to create more jobs in the United States and his trade deal with China.\n",
    "* **Topic 10** (_border wall great security north country korea immigration many good_): As expected, Trump is overrepresented in these results due to his aggressive border policies, including wanting to build a wall on the border between the USA and Mexico. Moreover, Trump is much more often associated with North Korea than Sanders would ever be.\n",
    "* **Topic 14** (_war president trump united states iran live world whitehouse must_): Out of all the topics, this one is the one where Sanders is the primary representative -- something which also surprises me. It seems that Trump himself was less interested about tweeting about the war in Iran than Sanders was in critiquing Trump for the way the White House and the United States involved themselves in the war."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4LxDb31D01p"
   },
   "source": [
    "## Large scale analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXmHmjaHI2fD"
   },
   "source": [
    "### Q8. Create a random sample of the whole dataset and visualize the topic distributions for the sampled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQdtQCJQiBUw"
   },
   "outputs": [],
   "source": [
    "# Create a sample of tweets\n",
    "\n",
    "from random import sample\n",
    "\n",
    "target_labels = sample(titles,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Af2ejUqBhtm9"
   },
   "outputs": [],
   "source": [
    "# Q8 (code)\n",
    "# Create a heatmap using the random sample\n",
    "# Tip: to display more than 20 tweets you have to change the values of `dim =` in sns.heatmap()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "def plot_categories_by_topics_heatmap(labels,\n",
    "                                      topic_distributions,\n",
    "                                      topic_keys,\n",
    "                                      output_path=None,\n",
    "                                      target_labels=None,\n",
    "                                      color_map = sns.cm.rocket_r,\n",
    "                                      dim=None):\n",
    "\n",
    "    # Combine the labels and distributions into a list of dictionaries.\n",
    "    dicts_to_plot = []\n",
    "    for _label, _distribution in zip(labels, topic_distributions):\n",
    "        if not target_labels or _label in target_labels:\n",
    "            for _topic_index, _probability in enumerate(_distribution):\n",
    "                dicts_to_plot.append({'Probability': float(_probability),\n",
    "                                      'Category': _label,\n",
    "                                      'Topic': 'Topic ' + str(_topic_index).zfill(2) + ': ' + ' '.join(topic_keys[_topic_index][:5])})\n",
    "\n",
    "    # Create a dataframe, format it for the heatmap function, and normalize the columns.\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(index='Category',\n",
    "                                     columns='Topic',\n",
    "                                     values='Probability')\n",
    "    df_norm_col=(df_wide-df_wide.mean())/df_wide.std()\n",
    "\n",
    "    # Show the final plot.\n",
    "    if dim:\n",
    "        plt.figure(figsize=dim)\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    ax = sns.heatmap(df_norm_col, cmap=color_map)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    plt.xticks(rotation=30, ha='left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_categories_by_topics_heatmap(titles,\n",
    "                                  topic_distributions,\n",
    "                                  topic_individual_words,\n",
    "                                  target_labels=target_labels,\n",
    "                                  color_map = 'YlOrBr', # funky colours taken from [https://matplotlib.org/stable/users/explain/colors/colormaps.html]\n",
    "                                 dim=(25,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trwbJnmGE2VR"
   },
   "source": [
    "### Q9. Interpret the heatmap\n",
    "Do you see any pattern in the probability distributions of topics for each politician?\n",
    "\n",
    "Are there topics that are more likely for one of the two politicians?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NPJXZUIIpor"
   },
   "source": [
    "_# Q9 (words)_\n",
    "\n",
    "Because the topics can get shifted around a little when re-running the code, I have added what a keywords show up per topic as I was answering this question. (The topics are the same as _Q5b_.) It should be noted that because I wanted to keep the topics the same as Q5b, and had messed around with the coherence scores prior to this, I was unable to re-run this code without getting different topic orders and/or slightly different topic words.\n",
    "\n",
    "Based on the randomly selected samples, Trump seems more inclined to talk about topics 0, 1, 2, 4-8, and 13; while Sanders' tweets allign themselves more with topics 3, 9, 12-14. Although it should be noted that topics 12-14 are not particularly exclusive to Sanders. It doesn't surprise me, however, that topic 9 (related to health care) is mostly associated with Sanders, as this was something he was very passionate about.\n",
    "\n",
    "However, it should be noted that most of the tweets in the heat map do originate from Trump, which is likely not the fault of the random sampling but reflective of Trump's infamous tweeting habits. As such, it seems to me that Sanders is somewhat underrepresented in the heatmap, if not the topics in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYZ9qZ2CEACM"
   },
   "source": [
    "### X2. Optional question 2\n",
    "(This question is not compulsory, it only allows you to get an extra point)\n",
    "\n",
    "Make the sample balanced, with 50 tweets by Trump and 50 by Sanders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVJBSdeP8MYK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X2 (code)\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "sanders_tweets = df[df['Username'] == 'SenSanders'] # Selects tweets by Sanders\n",
    "trump_tweets = df[df['Username'] == '@realDonaldTrump'] # Selects tweets by Trump\n",
    "\n",
    "sanders_sample = sanders_tweets.sample(n=min(50, len(sanders_tweets)), random_state=1) # Selects 50 random Sanders tweets using Pandas random\n",
    "trump_sample = trump_tweets.sample(n=min(50, len(trump_tweets)), random_state=1) # Selects 50 random Trump tweets using Pandas random\n",
    "    \n",
    "balanced_sample = pd.concat([sanders_sample, trump_sample])\n",
    "\n",
    "balanced_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVvkS8XaFCmL"
   },
   "source": [
    "### X3. Optional question 3\n",
    "(This question is not compulsory, it only allows you to get an extra point)\n",
    "\n",
    "Extend the analysis to all the tweets in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X3 (code and words)\n",
    "# Tip: plotting a heatmap for thousands of tweets is not practical.\n",
    "# Make a comparison based on the numerical values in the `df_norm_col` dataframe (see Week 6 notebook)\n",
    "\n",
    "training_data_titles = dict(zip(training_data, titles))\n",
    "training_data_original_text = dict(zip(training_data, original_texts))\n",
    "\n",
    "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
    "    \n",
    "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
    "\n",
    "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
    "        print(round(probability, 4), training_data_titles[document] + \"\\n\")\n",
    "    return\n",
    "\n",
    "## Code adapted from [https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/10-Topic-Modeling-CSV.html#display-top-titles-per-topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_titles_per_topic(topic_number=0, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 0\n",
    "display_top_titles_per_topic(topic_number=1, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 1\n",
    "display_top_titles_per_topic(topic_number=2, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 2\n",
    "display_top_titles_per_topic(topic_number=3, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 3\n",
    "display_top_titles_per_topic(topic_number=4, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 4\n",
    "display_top_titles_per_topic(topic_number=5, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 5\n",
    "display_top_titles_per_topic(topic_number=6, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 6\n",
    "display_top_titles_per_topic(topic_number=7, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 7\n",
    "display_top_titles_per_topic(topic_number=8, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 8\n",
    "display_top_titles_per_topic(topic_number=9, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 9\n",
    "display_top_titles_per_topic(topic_number=10, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 10\n",
    "display_top_titles_per_topic(topic_number=11, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 11\n",
    "display_top_titles_per_topic(topic_number=12, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 12\n",
    "display_top_titles_per_topic(topic_number=13, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 12\n",
    "display_top_titles_per_topic(topic_number=14, number_of_documents=5) # Shows the top 5 tweets with the highest probability of containing Topic 14"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
